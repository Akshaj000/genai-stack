# ðŸŒ± Embeddings

## Explanation

* Embeddings are numerical representations of data, typically used to represent words, sentences, or other objects in a vector space.
* In natural language processing (NLP), word embeddings are widely used to convert words into dense vectors. Each word is represented by a unique vector in such a way that semantically similar words have similar vectors.
* Popular word embedding methods include Word2Vec, GloVe, and FastText.
* Word embeddings are essential in various NLP tasks such as sentiment analysis, machine translation, and named entity recognition.
* They capture semantic relationships between words, allowing models to understand context and meaning.
* In addition to words, entire sentences or paragraphs can be embedded into fixed-length vectors, preserving the semantic information of the text.
* Sentence embeddings are useful for tasks like text classification, document clustering, and information retrieval

### Supported Embeddings:

Currently we support one Embedding platforms , they are:

* Langchain

By default you can get a embedding function which is HuggingFace&#x20;
